{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:33:44.503711Z","iopub.status.busy":"2023-04-11T15:33:44.503017Z","iopub.status.idle":"2023-04-11T15:39:53.452453Z","shell.execute_reply":"2023-04-11T15:39:53.451057Z","shell.execute_reply.started":"2023-04-11T15:33:44.503664Z"},"id":"55390e37","outputId":"c9c8815d-251f-4330-8a52-1d5beb2986ae","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-04-11 15:33:45--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.145.128, 74.125.143.128, 173.194.79.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.145.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3816687935 (3.6G) [application/zip]\n","Saving to: ‘nature_12K.zip.1’\n","\n","nature_12K.zip.1    100%[===================>]   3.55G  40.1MB/s    in 1m 42s  \n","\n","2023-04-11 15:35:28 (35.6 MB/s) - ‘nature_12K.zip.1’ saved [3816687935/3816687935]\n","\n","replace inaturalist_12K/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from math import ceil as ceil\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","import wandb\n","\n","# Download the dataset.\n","!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n","\n","# unzip the datqaset.\n","!unzip -q /kaggle/working/nature_12K.zip"]},{"cell_type":"markdown","metadata":{"id":"Gjv7BFoCyVv2"},"source":["The Following class prepData prepares the data with the option to augment or not\n","- it returns training data Loader, validation Data Loader, test Data Loader and the class Names dictionary\n","\n","- it takes a boolean(augment) as it's input which decides whether to augment the data or not"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:40:45.931389Z","iopub.status.busy":"2023-04-11T15:40:45.931019Z","iopub.status.idle":"2023-04-11T15:40:45.942327Z","shell.execute_reply":"2023-04-11T15:40:45.941156Z","shell.execute_reply.started":"2023-04-11T15:40:45.931356Z"},"id":"fa0s_Ish2Mjh","trusted":true},"outputs":[{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">devout-sweep-1</strong> at: <a href='https://wandb.ai/cs22m028/DLAssignment2/runs/ad0npmcn' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/runs/ad0npmcn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230411_153959-ad0npmcn/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def prepData(augment:bool):\n","    ''' Function to prepare the data using torch libraries for the purpose of training torch\n","        neural networks with relative ease.\n","        \n","        Using torch dataLoaders helps in memory management as well\n","        \n","        args : augment bool ---> True would enable data augmentation, False would disable.\n","        \n","        return : \n","            TrainDataLoader --> torch data loader wrapper for training dataset.\n","            ValDataLoader ----> torch data loader wrapper for validation data set.\n","            TestDataLoader ---> torch data loader wrapper for test data set.'''\n","    \n","    if augment == True:\n","        preProcess = transforms.Compose([\n","            transforms.Resize(size = (128,128)),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.RandomVerticalFlip(p=0.2),\n","            transforms.ToTensor()\n","        ])\n","    else:\n","        preProcess = transforms.Compose([\n","            transforms.Resize(size = (128,128)),\n","            transforms.ToTensor()\n","        ])\n","    print(\"loading the data into tensors ==============================\")\n","    trainData = datasets.ImageFolder(root = \"/kaggle/working/inaturalist_12K/train\",\n","                                    transform = preProcess,\n","                                    target_transform = None)\n","    classLabels = trainData.classes\n","    \n","    testData  = datasets.ImageFolder(root = \"/kaggle/working/inaturalist_12K/val\",\n","                                    transform = preProcess)\n","\n","    print(f\"train data : {trainData} and test data : {testData}\")\n","\n","    print(\"splitting into train and val ================================\")\n","    trainSplit = ceil(0.8*len(trainData))\n","    trainData, valData = torch.utils.data.random_split(trainData, [trainSplit, len(trainData) - trainSplit])\n","\n","    print(\"wrapping into train loader ==================================\")\n","\n","    trainDataLoader = torch.utils.data.DataLoader(trainData,\n","                                                shuffle=True,\n","                                                batch_size=32)\n","\n","    valDataLoader = torch.utils.data.DataLoader(valData,\n","                                                shuffle=True,\n","                                                batch_size=32)\n","\n","    testDataLoader = torch.utils.data.DataLoader(testData,\n","                                                shuffle=False,\n","                                                batch_size=32)\n","    \n","    print(\"loaders created for faster loading ===========================\")\n","\n","\n","\n","    return trainDataLoader, valDataLoader, testDataLoader, classLabels"]},{"cell_type":"markdown","metadata":{"id":"RPcZ5VPVyVv4"},"source":["The class for the model made from scratch\n","- named CNN Model, inherits nn.Module.\n","- must have a forward function for completion and successful training.\n","- Flexibility with respect to layer size, kernel size, and batchNormalization(add/not add)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:40:49.728177Z","iopub.status.busy":"2023-04-11T15:40:49.727791Z","iopub.status.idle":"2023-04-11T15:40:49.740079Z","shell.execute_reply":"2023-04-11T15:40:49.738651Z","shell.execute_reply.started":"2023-04-11T15:40:49.728144Z"},"id":"3kyOIIShqGDP","trusted":true},"outputs":[],"source":["class CNNModel(nn.Module):\n","    ''' CNN Model for classifying the images\n","    \n","        __init__ : creates a blueprint for the model\n","        forward  : forward propagation facilitated by Torch Layers.'''\n","\n","    def __init__(self, activation, kernels, inputShape: int, hiddenUnit: int, outputSize: int, dropOut: float, batchNorm: bool, factor: int):\n","        \n","        ''' initialize the model == inherit from nn.Module\n","            \n","            args : activation --> activation Function torch.nn.$SomeValidActivationFunction$\n","                   kernels ---> list conatining 5 kernel sizes that may be taken as input.\n","                   inputShape --> Number of Channels in input data.\n","                   hiddenUnit --> Filter size.\n","                   outputSize --> number of output channels.\n","                   batchNorm ---> boolean var to indicate whether to add batch normalization or not.\n","                   factor ------> int value used as multiplier for subsequent layers.\n","        '''\n","        super().__init__()\n","        self.hiddenUnit = hiddenUnit\n","        self.factor = factor\n","        self.batchNorm = batchNorm\n","\n","        self.conv_blocks = nn.ModuleList()    # ModuleList (Torch container) used to record specific layers.  \n","        self.batch_norms = nn.ModuleList()    # Using ModuleList shortens boiler plate code.\n","\n","        layerSize = [inputShape] + [self.hiddenUnit] + [self.factor * self.hiddenUnit] * 4\n","        for i in range(5):\n","            self.conv_blocks.append(nn.Conv2d(layerSize[i], layerSize[i+1], kernel_size=kernels[i], padding=2))  # add conv layers\n","            if self.batchNorm:\n","                self.batch_norms.append(nn.BatchNorm2d(layerSize[i+1])) # add Batch normalization only if specified.\n","\n","        self.activate = activation  # add activation function (taken as input).\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # max pool layer (as and when to be used).\n","        self.drop = nn.Dropout(p=dropOut)  # drop out layer for reducing over fitting.\n","        \n","        # DenseBlock containing a flattening layer, dense layer.\n","        self.DenseBlock = nn.Sequential(\n","            nn.Flatten(),\n","            nn.LazyLinear(out_features=1024, bias=True, device=None, dtype=None), # LazyLinear used for calculation of in_features.\n","            nn.Dropout(p=dropOut),\n","            nn.Linear(in_features=1024, out_features=outputSize)\n","        )\n","\n","    def forward(self, x):\n","        ''' Forward Propagation\n","            x ---> tensor denoting input value.\n","            return x ---> prediction value.'''\n","        for i in range(5):\n","            x = self.conv_blocks[i](x)\n","            if self.batchNorm and i < 5:\n","                x = self.batch_norms[i](x)\n","            x = self.activate(x)\n","            x = self.pool(x)\n","\n","        x = self.drop(x)\n","        x = self.DenseBlock(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:40:53.545684Z","iopub.status.busy":"2023-04-11T15:40:53.544707Z","iopub.status.idle":"2023-04-11T15:40:53.551611Z","shell.execute_reply":"2023-04-11T15:40:53.550392Z","shell.execute_reply.started":"2023-04-11T15:40:53.545646Z"},"id":"v8o0a9XSyVv5","trusted":true},"outputs":[],"source":["def accuracy(y_true, y_pred):\n","    ''' accuracy Function for calculating the percentage of y_true[i] == y_pred[i]\n","        args : y_true ---> int actual value/ label(s) of for the input(s).\n","        return : accuracy ---> float [0,100] The accuracy of the batch.\n","    '''\n","    correct = torch.eq(y_true,y_pred).sum().item()\n","    accuracy = 0.0\n","    accuracy = correct/(len(y_true))*100\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T16:47:05.208411Z","iopub.status.busy":"2023-04-11T16:47:05.207991Z","iopub.status.idle":"2023-04-11T16:47:05.222939Z","shell.execute_reply":"2023-04-11T16:47:05.221944Z","shell.execute_reply.started":"2023-04-11T16:47:05.208377Z"},"id":"XyFNQ3Htw3RE","trusted":true},"outputs":[],"source":["def fit(model, trainDataLoader, valDataLoader, epochs, device, loss_fn, optimizer):\n","    ''' Function for training the model on the data set.\n","        args --->\n","            model -> CNNModule object \n","            trainDataLoader --> torch dataLoader wrapper containing training set.\n","            valDataLoader --> torch dataLoader wrapper containing validation set.\n","            epochs --> int, number of epochs.\n","            device --> whether cpu or cuda.\n","            loss_fn ---> loss Function used.\n","            optimizer --> optimizer function used.\n","            \n","        return model --> CNN Module object with updated weights.\n","    '''\n","    for epoch in tqdm(range(epochs)):\n","        train_loss = 0\n","        train_acc = 0\n","        for batch, (X,y) in enumerate(trainDataLoader):\n","            X,y = X.to(device), y.to(device)\n","            model.train()\n","            y_pred = model(X)\n","            loss = loss_fn(y_pred, y)\n","            train_loss += loss\n","            train_acc += accuracy(y_true=y, y_pred=y_pred.argmax(dim=1))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch%50 == 0:\n","                print(f\"went through {batch}/{len(trainDataLoader)} samples\")\n","            torch.cuda.empty_cache()\n","\n","        train_loss /= len(trainDataLoader)\n","        train_acc /= len(trainDataLoader)\n","    \n","        val_loss = 0.0\n","        val_acc = 0\n","        model.eval()\n","        with torch.inference_mode():\n","            for X,y in valDataLoader:\n","                X,y = X.to(device), y.to(device)\n","                val_pred = model(X)\n","                val_loss += loss_fn(val_pred, y)\n","                val_acc += accuracy(y_true=y, y_pred=val_pred.argmax(dim=1))\n","            val_acc /= len(valDataLoader)\n","            val_loss /= len(valDataLoader)\n","        \n","        #wandb.log({\"TrainingLoss\" : train_loss, \"ValidationLoss\" : val_loss, \"TrainingAccuracy\" : train_acc, \"ValidationAccuracy\" : val_acc, \"epoch\": epoch})\n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_acc}, validation loss: {val_loss}, validation accuracy: {val_acc}\\n\")\n","    torch.save(model, \"/kaggle/working/vit_16_model.pth\")\n","\n","    return model\n","\n","def eval(model, testDataLoader):\n","    ''' Function for evaluating the training on unseen test Dataset.\n","        args --> testDataLoader torch DataLoader object for easy loading/unloading.\n","    '''\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    test_loss = 0.0\n","    test_acc = 0\n","    model.eval()\n","    with torch.inference_mode():\n","        for X,y in testDataLoader:\n","            X,y = X.to(device), y.to(device)\n","            test_pred = model(X)\n","            test_loss += loss_fn(test_pred, y)\n","            test_acc += accuracy(y_true=y, y_pred=test_pred.argmax(dim=1))\n","        test_acc /= len(testDataLoader)\n","        test_loss /= len(testDataLoader)\n","    print(f\"test accuracy is {test_acc} and test loss os {test_loss}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:40:58.814347Z","iopub.status.busy":"2023-04-11T15:40:58.813718Z","iopub.status.idle":"2023-04-11T15:40:58.825117Z","shell.execute_reply":"2023-04-11T15:40:58.823702Z","shell.execute_reply.started":"2023-04-11T15:40:58.814310Z"},"id":"M7C2ThVtB0Kx","trusted":true},"outputs":[],"source":["def masterTrainer(trainDataLoader, valDataLoader, testDataLoader, learningRate, kernels, layerSize, dropOut, batchNorm, activation, factor, epochs):\n","    ''' function to start the training and facilitate wandb logging.\n","        args ->\n","            trainDataLoader --> torch dataLoader wrapper containing training set.\n","            valDataLoader --> torch dataLoader wrapper containing validation set.\n","            testDataLoader --> torch dataLoader wrapper containing test dataset.\n","            \n","            learningRate ---> int, learning rate,\n","            kernels --> list of kernel sizes, one for each convolutional layer.\n","            layerSize --> filter size of first convolutional layer\n","            factor --> multiplier to number of filters for subsequent training.\n","            epochs --> int, number of epochs.\n","    '''\n","    activations = {\n","    \"relu\" : torch.nn.ReLU(),\n","    \"gelu\" : torch.nn.GELU(),\n","    \"silu\" : torch.nn.SiLU(),\n","    \"mish\" : torch.nn.Mish()\n","    }\n","\n","    #import wandb    \n","    #wandb.init(project=\"DLAssignment2\", entity=\"cs22m028\")\n","    #wandb.run.name = \"config_\"+str(optimizer)+\"_\"+str(layerSize)+\"_\"+str(decay)+\"_\"+str(opt)+\"_\"+str(batchNorm)+\"_\"+str(dropOut)+\"_\"+str(activation)    \n","    activate= activations[activation]\n","\n","    model_0 = CNNModel(activate, kernels, inputShape=3, hiddenUnit=layerSize,outputSize=10, dropOut = dropOut, batchNorm=True, factor = factor)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model_0.to(device)\n","    #from helper_functions import accuracy_fn as accuracy # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(params = model_0.parameters(),lr=learningRate)\n","    print(model_0)\n","    model_0 = fit(model_0, trainDataLoader, valDataLoader, epochs, device, loss_fn, optimizer)\n","    \n","    eval(model_0, testDataLoader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T18:10:56.135978Z","iopub.status.busy":"2023-04-11T18:10:56.135098Z","iopub.status.idle":"2023-04-11T18:11:26.030784Z","shell.execute_reply":"2023-04-11T18:11:26.029668Z","shell.execute_reply.started":"2023-04-11T18:10:56.135937Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading the data into tensors ==============================\n","train data : Dataset ImageFolder\n","    Number of datapoints: 9999\n","    Root location: /kaggle/working/inaturalist_12K/train\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n","               RandomHorizontalFlip(p=0.5)\n","               RandomVerticalFlip(p=0.2)\n","               ToTensor()\n","           ) and test data : Dataset ImageFolder\n","    Number of datapoints: 2000\n","    Root location: /kaggle/working/inaturalist_12K/val\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n","               RandomHorizontalFlip(p=0.5)\n","               RandomVerticalFlip(p=0.2)\n","               ToTensor()\n","           )\n","splitting into train and val ================================\n","wrapping into train loader ==================================\n","loaders created for faster loading ===========================\n","test accuracy is 39.732142857142854 and test loss os 1.8127118349075317\n"]}],"source":["\n","trainDataLoader, valDataLoader, testDataLoader, classNames = prepData(augment=True)\n","\n","model_0 = torch.load(\"vit_16_model.pth\")\n","\n","loss_fn = nn.CrossEntropyLoss()\n","#optimizer = torch.optim.Adam(params = model_0.parameters(),lr=0)\n","\n","eval(model_0, testDataLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T18:01:36.278708Z","iopub.status.busy":"2023-04-11T18:01:36.277561Z","iopub.status.idle":"2023-04-11T18:02:16.121178Z","shell.execute_reply":"2023-04-11T18:02:16.120038Z","shell.execute_reply.started":"2023-04-11T18:01:36.278659Z"},"trusted":true},"outputs":[{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"]},{"data":{"text/html":["Finishing last run (ID:ybwdj0n5) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">config_activation=mish_epochs=20_dropOut=0.2_batchSize=32_filterSize=32_batchNorm=True_augment=True_learningRate=0.0001</strong> at: <a href='https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230411_175940-ybwdj0n5/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:ybwdj0n5). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.14.2 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.14.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_180136-ybwdj0n5</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5' target=\"_blank\">config_activation=mish_epochs=20_dropOut=0.2_batchSize=32_filterSize=32_batchNorm=True_augment=True_learningRate=0.0001</a></strong> to <a href='https://wandb.ai/cs22m028/DLAssignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22m028/DLAssignment2' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["randomImageModeler(model_0, testDataLoader, classNames)"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T18:01:32.538682Z","iopub.status.busy":"2023-04-11T18:01:32.538303Z","iopub.status.idle":"2023-04-11T18:01:32.548531Z","shell.execute_reply":"2023-04-11T18:01:32.547454Z","shell.execute_reply.started":"2023-04-11T18:01:32.538647Z"},"id":"El5gqsyrzgoz","trusted":true},"outputs":[],"source":["def randomImageModeler(model, testDataLoader, classNames):\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    wandb.init(project=\"DLAssignment2\", entity=\"cs22m028\")\n","    model.to(\"cpu\")\n","    model.eval()\n","    count = 0\n","    images = []\n","    with torch.inference_mode():\n","        for X,y in testDataLoader:\n","            #X,y = X.to(device), y.to(device)\n","            prediction = model(X)\n","            # That is it...\n","            # print the image.\n","            for i in range(30):\n","                img = X[i].permute(1, 2, 0).numpy()\n","                image = wandb.Image(img, caption= classNames[prediction[i].argmax(dim=0)])\n","                images.append(image)\n","            break\n","        wandb.log({\"Some predictions....\": images})\n","    # It is done."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:43:37.349550Z","iopub.status.busy":"2023-04-11T15:43:37.349186Z","iopub.status.idle":"2023-04-11T15:43:37.361769Z","shell.execute_reply":"2023-04-11T15:43:37.360526Z","shell.execute_reply.started":"2023-04-11T15:43:37.349516Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]}],"source":["model_temp = CNNModel(nn.GELU, [5,3,3,3,3], inputShape=3, hiddenUnit=32,outputSize=10, dropOut = 0.5, batchNorm=True, factor = 2)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-11T15:41:03.629622Z","iopub.status.busy":"2023-04-11T15:41:03.628667Z","iopub.status.idle":"2023-04-11T15:41:03.638864Z","shell.execute_reply":"2023-04-11T15:41:03.637678Z","shell.execute_reply.started":"2023-04-11T15:41:03.629571Z"},"id":"ssI1H_s7sK_m","trusted":true},"outputs":[],"source":["def testIt():\n","    ''' wandb trainer for initializing runs, loading data and running the entire training/ testing process.'''\n","    wandb.init(project=\"DLAssignment2\", entity=\"cs22m028\")\n","    wandb.run.name=\"config_activation=\"+str(wandb.config.activation)+\"_epochs=\"+str(wandb.config.epochs)+\"_dropOut=\"+str(wandb.config.dropOut)+\"_batchSize=\"+str(wandb.config.batchSize) + \"_filterSize=\"+str(wandb.config.filterSize)+\"_batchNorm=\"+str(wandb.config.batchNorm)+\"_augment=\"+str(wandb.config.augmentation)+\"_learningRate=\"+str(wandb.config.learningRate)\n","    trainDataLoader, valDataLoader, testDataLoader, classLabels = prepData(augment=wandb.config.augmentation)\n","    #trainDataLoader, valDataLoader, testDataLoader, classLabels = prepData(augment=True)\n","    #masterTrainer(trainDataLoader, valDataLoader, testDataLoader, 0.001, 0.00001, [5,5,3,3,3], 32, \"adam\", \"cross\", 0.2, True, \"gelu\", 1)\n","    masterTrainer(trainDataLoader, valDataLoader, testDataLoader, wandb.config.learningRate, wandb.config.kernels, wandb.config.filterSize, wandb.config.dropOut, wandb.config.batchNorm, wandb.config.activation, wandb.config.factor, wandb.config.epochs)\n","    #masterTrainer(trainDataLoader, valDataLoader, testDataLoader, config.wandb.learningRate, config.wandb.decay, config.wandb.kernels, config.wandb.layerSize, config.wandb.dropOut, config.wandb.batchNorm, config.wandb.activation, config.wandb.factor)"]},{"cell_type":"markdown","metadata":{"id":"L3Stj8xKVEd8"},"source":["Here, the possible parameters for the sweeps have been swapped by the best parameters only, and therefore, only one run will happen and the images will get logged.\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"referenced_widgets":["73aad317e8ad42f6bbbadb3e47852ccb","3fb1a5e31e224ab19ecfb397d2a58d1b"]},"execution":{"iopub.execute_input":"2023-04-11T15:45:59.945318Z","iopub.status.busy":"2023-04-11T15:45:59.944943Z","iopub.status.idle":"2023-04-11T16:40:57.404083Z","shell.execute_reply":"2023-04-11T16:40:57.402806Z","shell.execute_reply.started":"2023-04-11T15:45:59.945284Z"},"id":"t_4U_ofB7FJo","outputId":"5bd66bc5-ab40-452f-a5ef-942353375694","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"name":"stdout","output_type":"stream","text":["Create sweep with ID: vnlswd11\n","Sweep URL: https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 895tp3tc with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n","\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: True\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchNorm: True\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropOut: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tfilterSize: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [5, 5, 3, 3, 3]\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"]},{"data":{"text/html":["wandb version 0.14.2 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.14.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_154603-895tp3tc</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22m028/DLAssignment2/runs/895tp3tc' target=\"_blank\">scarlet-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m028/DLAssignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22m028/DLAssignment2' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22m028/DLAssignment2/runs/895tp3tc' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/runs/895tp3tc</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["loading the data into tensors ==============================\n","train data : Dataset ImageFolder\n","    Number of datapoints: 9999\n","    Root location: /kaggle/working/inaturalist_12K/train\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n","               RandomHorizontalFlip(p=0.5)\n","               RandomVerticalFlip(p=0.2)\n","               ToTensor()\n","           ) and test data : Dataset ImageFolder\n","    Number of datapoints: 2000\n","    Root location: /kaggle/working/inaturalist_12K/val\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n","               RandomHorizontalFlip(p=0.5)\n","               RandomVerticalFlip(p=0.2)\n","               ToTensor()\n","           )\n","splitting into train and val ================================\n","wrapping into train loader ==================================\n","loaders created for faster loading ===========================\n","CNNModel(\n","  (conv_blocks): ModuleList(\n","    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","  )\n","  (batch_norms): ModuleList(\n","    (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (activate): Mish()\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (drop): Dropout(p=0.2, inplace=False)\n","  (DenseBlock): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): LazyLinear(in_features=0, out_features=1024, bias=True)\n","    (2): Dropout(p=0.2, inplace=False)\n","    (3): Linear(in_features=1024, out_features=10, bias=True)\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6275f17770b94c178a0e8aa498e79982","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 2.1353471279144287, Train accuracy: 22.8625, validation loss: 2.0054874420166016, validation accuracy: 28.895502645502646\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 2.0064029693603516, Train accuracy: 28.9375, validation loss: 1.9214439392089844, validation accuracy: 33.62103174603175\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.93743896484375, Train accuracy: 31.325, validation loss: 1.8797141313552856, validation accuracy: 34.74867724867725\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.8730131387710571, Train accuracy: 34.475, validation loss: 1.8663069009780884, validation accuracy: 35.81018518518518\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.8344883918762207, Train accuracy: 34.675, validation loss: 1.8448214530944824, validation accuracy: 36.739417989417994\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.7800477743148804, Train accuracy: 37.6625, validation loss: 2.027337074279785, validation accuracy: 29.738756613756614\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.7481117248535156, Train accuracy: 38.4375, validation loss: 1.850728988647461, validation accuracy: 35.05291005291006\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.7092822790145874, Train accuracy: 40.7625, validation loss: 1.8284595012664795, validation accuracy: 35.40013227513228\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.6571322679519653, Train accuracy: 42.4875, validation loss: 1.764482021331787, validation accuracy: 38.08531746031746\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.6217041015625, Train accuracy: 43.2375, validation loss: 1.7318278551101685, validation accuracy: 40.03968253968254\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.5891854763031006, Train accuracy: 45.475, validation loss: 1.7852839231491089, validation accuracy: 38.73015873015873\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.5701904296875, Train accuracy: 46.05, validation loss: 1.7367595434188843, validation accuracy: 39.82142857142857\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.5357171297073364, Train accuracy: 46.85, validation loss: 1.8782941102981567, validation accuracy: 36.30621693121693\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.5262750387191772, Train accuracy: 47.3875, validation loss: 1.7638251781463623, validation accuracy: 38.02910052910053\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.4874241352081299, Train accuracy: 48.3, validation loss: 1.7822234630584717, validation accuracy: 39.04100529100529\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.4715659618377686, Train accuracy: 49.525, validation loss: 1.7478793859481812, validation accuracy: 39.43121693121693\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.4426189661026, Train accuracy: 50.3375, validation loss: 1.7217015027999878, validation accuracy: 41.160714285714285\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.4195820093154907, Train accuracy: 51.2625, validation loss: 1.8479679822921753, validation accuracy: 37.63227513227513\n","\n","went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n","went through 150/250 samples\n","went through 200/250 samples\n","Train loss: 1.3925849199295044, Train accuracy: 52.5625, validation loss: 1.8445181846618652, validation accuracy: 37.00066137566137\n","\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a9a7066dd614087a40c3622db6020f5","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">scarlet-sweep-1</strong> at: <a href='https://wandb.ai/cs22m028/DLAssignment2/runs/895tp3tc' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/runs/895tp3tc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230411_154603-895tp3tc/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Run 895tp3tc errored: NameError(\"name 'valDataLoader' is not defined\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 895tp3tc errored: NameError(\"name 'valDataLoader' is not defined\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ybwdj0n5 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n","\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: True\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchNorm: True\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropOut: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tfactor: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tfilterSize: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tkernels: [5, 5, 3, 3, 3]\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"]},{"data":{"text/html":["wandb version 0.14.2 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.14.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_163913-ybwdj0n5</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5' target=\"_blank\">chocolate-sweep-2</a></strong> to <a href='https://wandb.ai/cs22m028/DLAssignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22m028/DLAssignment2' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/sweeps/vnlswd11</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5' target=\"_blank\">https://wandb.ai/cs22m028/DLAssignment2/runs/ybwdj0n5</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["loading the data into tensors ==============================\n","train data : Dataset ImageFolder\n","    Number of datapoints: 9999\n","    Root location: /kaggle/working/inaturalist_12K/train\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n","               RandomHorizontalFlip(p=0.5)\n","               RandomVerticalFlip(p=0.2)\n","               ToTensor()\n","           ) and test data : Dataset ImageFolder\n","    Number of datapoints: 2000\n","    Root location: /kaggle/working/inaturalist_12K/val\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n","               RandomHorizontalFlip(p=0.5)\n","               RandomVerticalFlip(p=0.2)\n","               ToTensor()\n","           )\n","splitting into train and val ================================\n","wrapping into train loader ==================================\n","loaders created for faster loading ===========================\n","CNNModel(\n","  (conv_blocks): ModuleList(\n","    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","  )\n","  (batch_norms): ModuleList(\n","    (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (activate): Mish()\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (drop): Dropout(p=0.2, inplace=False)\n","  (DenseBlock): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): LazyLinear(in_features=0, out_features=1024, bias=True)\n","    (2): Dropout(p=0.2, inplace=False)\n","    (3): Linear(in_features=1024, out_features=10, bias=True)\n","  )\n",")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3434607d5c894eb494c8a12448671f02","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["went through 0/250 samples\n","went through 50/250 samples\n","went through 100/250 samples\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"]}],"source":["if __name__ == '__main__':\n","    wandb.login()\n","    sweep_config = {\n","        'method': 'bayes'\n","        }\n","\n","    metric = {\n","        'name': 'val_acc',\n","        'goal': 'maximize'   \n","        }\n","\n","    sweep_config['metric'] = metric\n","\n","\n","    # initialize the dictionaty with only the best parameters.\n","\n","    parameters_dict = {\n","        'epochs' : {\n","            'values':[20]\n","        },\n","        'factor':{\n","            'values':[2]},\n","        'filterSize': {\n","            'values': [32]\n","            },\n","        'dropOut' : {\n","            'values' : [0.2]\n","            },\n","        'batchSize' : {\n","            'values' : [32]\n","            },\n","        'activation' : {\n","            'values' : ['mish']\n","            },\n","        'augmentation' : {\n","            'values':[True]\n","        },\n","        'batchNorm' : {\n","            'values' : [True]\n","            },\n","        'kernels' : {\n","            'values' : [[5, 5, 3, 3, 3]]\n","            },\n","        'learningRate' : {\n","            'values' : [0.0001]\n","         }\n","        }\n","\n","    sweep_config['parameters'] = parameters_dict\n","\n","    # intitialize sweep id.\n","    sweep_id = wandb.sweep(sweep_config, project= \"DLAssignment2\")\n","    wandb.agent(sweep_id,project= \"DLAssignment2\" , function = testIt)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
